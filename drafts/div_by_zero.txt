This is the speed of light.

299 792 458

It is a relatively small number. It has nine digits and I can easily write it on this small sheet of paper.

But what if I only have like half of the sheet?

If we're ok with some small error, let's say one tens of a percent, then we can say that the speed of light is approximately this:

300 000 000

It's still nine digits but we can also write it as:

3*10^8

Or, in ingeneering notation,

3e8

It's the same number, the same power of ten, it just takes fewer symbols to write it down.

Now 3e8 represents the speed of light. With some small error, of course.

But it also represents a lot of other numbers. In fact, it represents every number that fits the error threshold. So it's not a number but a range.

Digital computers mostly keep numbers in the same way. They store a small amount of binary digits for the number and even a smaller amount of binary digits to indicate the multilpying power of two. In this way they can operate both very large and very small numbers. But of course, the number of digits is finite so there are numbers so large they can not be written at all. They just don't fit the digits we have.

And the same goes to the small numbers. There are numbers so small that we can't write them down in engineering notation, the right part would be too large.

With digital computers, all that extra small numbers are represented by zero.

Just like 3e8 represents a range of numbers, a 0 represents not a single number but every number that is so small you don't have enough digits to write down how small it is.

So the number representing zero might be an actual zero but it might just not be an actual zero. So when we ask a computer to divide a number by zero we're actually telling it, divide it by some arbitrary real small number. And it does. And the result is the arbitrary very large number. 

It's called infinite but just as number zero doesn't actually represent zero but all the ultra-small numbers, infinite doesn't represent infinity but rather all the numbers that are so large you don't have enough digits to write down how large they are.

But why do we have to keep our numbers small? Why is is importrant to fit into some relatively small amoun of digits?

And the answer is, the speed of light.

In computing, especially in high-performant computing when you measure latencies in nanoseconds, the light isn't really very-very fast. Here I'll show you.

In one nanosecond, the light travels only about 30 cm far.

And that's it. That's the essential limitation of electronic computers. You can make your processor faster by making the transistors smaller but this also has its limit. You can't make subatomic transistors.

And that's why we have to be resourceful with our numbers. Not with the real numbers of course but with the numbers we crunch with computers. And having division by zero allowed is a small price to pay realy.
